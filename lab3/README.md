# Лабораторная работа №3
# Разработка сверточных нейронных сетей

+ [Входные данные](#Format_input)
+ [Описание программной реализации](#Description)
+ [Архитектуры нейронных сетей](#NN_architecture)
+ [Результаты экспериментов](#Results)


## <a name="Format_input"></a>	Входные данные
Входным сигналом для нейронных сетей, описанных ниже, являются нормированные изображения 28x28 пикселей.


## <a name="NN_architecture"></a>	Архитектуры нейронных сетей
В качестве архитектур использовались модели с 1-3 сверточными скрытыми слоями с двумерными свертками и полносвязным выходным слоем в конце. 
В соответствии с [таблицей](#Table) варьировались количество слоев, количество фильтров, размеры ядер и функции активаций. Для всех 
скрытых сверточных слоев использовалась одинаковая функция активации. На выходном слое всегда использовалась функция 
активации softmax, так как классов 10.

Используемые архитектуры сетей:

<a name="Table"></a>

| Количество слоёв | Количество фильтров | Размеры ядер | Функция активации слоёв|
|:----------------:|:-------------------:|:------------:|:----------------------:|
| 1 | 32 | (3,3) | relu |
| 1 | 32 | (5,5) | relu |
| 2 | 32, 32 | (3,3), (5,5) | relu |
| 2 | 32, 32 | (5,5), (3,3) | relu |
| 3 | 32, 32, 32 | (3,3), (3,3), (3,3) | relu |
| 1 | 32 | (3,3) | sigmoid |
| 1 | 32 | (5,5) | sigmoid |
| 2 | 32, 32 | (3,3), (5,5) | sigmoid |
| 2 | 32, 32 | (5,5), (3,3) | sigmoid |
| 3 | 32, 32, 32 | (3,3), (3,3), (3,3) | sigmoid |


В качестве функции потерь использовалась перекрестная энтропия:

![](https://latex.codecogs.com/gif.latex?E%28w%29%3D-%5Csum%5Climits_%7Bj%3D1%7D%5EMy_j%5Cln%7Bu_j%7D)
    
где ![](https://latex.codecogs.com/gif.latex?y_j) – ожидаемый выход (метки),

![](https://latex.codecogs.com/gif.latex?u_j) – выход сети.

Качество сети оценивалось через точность - отношение количества меток, совпавших с предсказанными к числу примеров:

![](https://latex.codecogs.com/gif.latex?\frac{I(y_j=u_j)}{N},j=\overline{1,N})


## <a name="Description"></a>	Описание программной реализации
[dataset.py]() содержит методы для обработки входных данных:

+ load_dataset() загружает набор данных, нормирует x и приводит y к one-hot кодированию.

[models.py]() содержит методы для создания моделей:

+ build_dense_model_1() строит сверточную модель в соответствии с параметрами из аргументов.
+ generate_model_zoo() создаёт список моделей разных архитектур для экспериментов.

[experiments.py]() содержит методы для проведения экспериментов на моделях:

+ save_history_img() сохраняет график обучения
+ calculate_accuracy() считает точность модели
+ train_models() обучает каждую модель из входного списка моделей. Возвращает модели с весами, которые показали лучшую точность на валидации
+ test_models() тестирует кадую модель из входного списка

## <a name="Results"></a>	Результаты экспериментов
Обучение отанавливается при достижении 150 эпох или если в течении 5 эпох подряд ошибка на валидационном множестве не изменяется.

| Архитектура сети | Время обучения, эпохи | Качество решения, точность|
|:----------------:|:---------------------------:|:----------------------:|
| 32 // (3,3) | 150 | 0.8442 |
| 32 // (5,5) | 75 | 0.8201 |
| 32 // (3,3), 32 // (5,5) | 100 | 0.8417 |
| 32 // (5,5), 32 // (3,3) | 50 | 0.7932 |
| 32 // (3,3), 32 // (3,3), 32 // (3,3) | 35 | 0.7489 |
| 32 // (3,3) | 150 | 0.8011 |
| 32 // (5,5) | 150 | 0.7956 |
| 32 // (3,3), 32 // (5,5) | 150 | 0.7440 |
| 32 // (5,5), 32 // (3,3) | 150 | 0.7445 |
| 32 // (3,3), 32 // (3,3), 32 // (3,3) | 75 | 0.1483 |

Можно видеть, что функция активации sigmoid в рамках данной задачи показывает результаты хуже, чем relu. 
