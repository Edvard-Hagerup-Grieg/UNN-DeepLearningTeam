# Лабораторная работа №2
# Разработка полностью связанных нейронных сетей

+ [Входные данные](#Format_input)
+ [Архитектуры нейронных сетей](#NN_architecture)
+ [Описание программной реализации](#Description)
+ [Результаты экспериментов](#Results)


## <a name="Format_input"></a>	Входные данные
Загрузка и предобработка данных реализована в файле [dataset.py](https://github.com/Edvard-Hagerup-Grieg/UNN-DeepLearningTeam/blob/report_lab2/lab2/dataset.py).


Для использования в качестве входного сигнала описанных ниже нейронных сетей, нормированные изображений 28x28 пикселей представляются в виде вектора длины 784.


## <a name="NN_architecture"></a>	Архитектуры нейронных сетей
В качестве архитектур использовались 4 лучшие архитектуры из лабораторных работ 2 и 3. Модели для обучения без учителя состояли из двух частей: энкодера и декодера. Энкодером служила основная архитектура без последнего классифицирующего слоя, декодер строился симметрично энкодеру.

Используемые полносвязные архитектуры автокодировщиков:

| Количество слоёв | Количество нейронов в слоях (энкодера//декодера) | Функция активации |
|:----------------:|:---------------------------:|:--------:|
| 4 | 512, 256 // 256, 512 | relu |
| 6 | 512, 256, 128 // 128, 256, 512 | linear |
| 4 | 512, 256 // 256, 512 | linear |
| 6 | 512, 256, 128 // 128, 256, 512 | relu |

В качестве функции потерь для автокодировщика использовалась среднеквадратичная ошибка.

После обучения автокодировщика, к его энкодеру присоединялся полносвязный классифицирующий слой с функцией активации softmax.
Полученная классифицирующая сеть обучалась с перекрестной энтропией в качестве функции потерь.

## <a name="Description"></a>	Описание программной реализации

### dataset.py содержит методы для обработки входных данных:

load_dataset загружает набор данных, нормирует, приводит x к векторному виду, а y к one-hot кодированию.

### models.py содержит методы для создания моделей:

build_simple_autoencoder строит симметричный полносвязный автокодировщик

generate_model_zoo_unsupervised создаёт список моделей классификаторов и соответствующих им автокодировщиков

### experiments.py содержит методы для проведения экспериментов на моделях

save_history_img сохраняет график обучения.

train_models обучает список классифицирующих моделей. Возвращает модели с весами, которые показали лучшую точность на валидации

test_models тестирует список моделей.

train_unsupervised_models обучает список автокодировщиков.

## <a name="Results"></a>	Результаты экспериментов
Автокодировщик обучался на 50 эпохах, после чего, его энкодер использовался для построения классификатора. 
Обучение модели с классификатором останавливалось либо при достижении 150 эпохи, либо при падении точности в течении 5 эпох.

Результаты полносвязных моделей с предобученным автокодировщиком

| Архитектура сети| Время обучения, эпохи | Качество решения, точность| Точность в экспериментах без предобучения
|:----------------:|:---------------------------:|:----------------------:|:-----:|
| 512, 256, relu | 150 | 0.8449 | 0.8472 |
| 512, 256, 128, linear | 150 | 0.8401 | 0.8405 |
| 512, 256, linear | 150 | 0.8373 |0.8376|
| 512, 256, 128, relu | 80 | 0.8343 | 0.8356 |

По результатам на полносвязных моделях видно, что в большинстве экспериментов предобучение сетей без учителя не улучшает результаты. При этом, оно также не влияет на время обучения: модель (512, 256, 128, relu) без предобучения обучалась 70 эпох.
