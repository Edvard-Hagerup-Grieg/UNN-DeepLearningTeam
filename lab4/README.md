# Лабораторная работа №2
# Разработка полностью связанных нейронных сетей

+ [Входные данные](#Format_input)
+ [Архитектуры нейронных сетей](#NN_architecture)
+ [Результаты экспериментов](#Results)
+ [Описание программной реализации](#Description)


## <a name="Format_input"></a>	Входные данные
Загрузка и предобработка данных реализована в файле [dataset.py](https://github.com/Edvard-Hagerup-Grieg/UNN-DeepLearningTeam/blob/report_lab2/lab2/dataset.py).


Для использования в качестве входного сигнала описанных ниже нейронных сетей, нормированные изображений 28x28 пикселей представляются в виде вектора длины 784.


## <a name="NN_architecture"></a>	Архитектуры нейронных сетей
В качестве архитектур для предобучения без учителя использовались свёрточные и полносвязные сети с функцией активации relu
на всех слоях. Количество нейронов в слоях дэкодера и энкодера симметрично.

Используемые полносвязные архитектуры автокодировщиков:

| Количество слоёв | Количество нейронов в слоях (энкодера//декодера) |
|:----------------:|:---------------------------:|
| 2 | 512 // 512 |
| 6 | 512, 256 // 256, 512 |
| 6 | 512, 256, 64 // 64, 256, 512 |

В качестве функции потерь для автокодировщика использовалась среднеквадратичная ошибка

После обучения автокодировщика, его энкодер использовался как вход для классифицирующей модели. Количество нейронов классификатора
зависит от размера скрытого пространства автокодировщика. Классифицирующие модели также использовали
relu в качестве функции активации скрытых слоёв. Функцией активации выходного слоя был softmax.

Используемые полносвязные архитектуры классификаторов:

| Количество слоёв | Количество нейронов в слоях |
|:----------------:|:---------------------------:|
| 1 | input / 2 |
| 2 | input / 2, input / 4 |
| 3 | input / 2, input / 4, input / 8 |

где input - размер выхода энкодера.

## <a name="Description"></a>	Описание программной реализации

### dataset.py содержит методы для обработки входных данных:

load_dataset загружает набор данных, нормирует, приводит x к векторному виду, а y к one-hot кодированию.

### models.py содержит методы для создания моделей:

build_dense_model_1 строит полносвязную модель в соответствии с параметрами из аргументов.

generate_model_zoo создаёт список моделей разных архитектур для экспериментов.

### experiments.py содержит методы для проведения экспериментов на моделях

save_history_img сохраняет график обучения.

calculate_accuracy считает точность модели.

train_models обучает список моделей. Возвращает модели с весами, которые показали лучшую точность на валидации

test_models тестирует список моделей.

## <a name="Results"></a>	Результаты экспериментов
Автокодировщик обучался на 50 эпохах, после чего, его энкодер использовался для построения над ним классификатора. 
Обучение модели с классификатором останавливалось либо при достижении 150 эпохи, либо при падении точности в течении 5 эпох.

Также проводились эксперименты без предварительного предобучения автокодировщика, для сравнения результатов.

Результаты модели с предобученным автокодировщиком

| Архитектура сети(энкодер//классификатор) | Время обучения, эпохи | Качество решения, точность|
|:----------------:|:---------------------------:|:----------------------:|
| 512, 256, 128 // 64 | 150 | 0.8328 |
| 512, 256, 128 // 64, 32 | 150 | 0.8328 |
| 512, 256, 128 // 64, 32, 16 | 150 | 0.8328 |
| 512, 256 // 128, 64, 32 | 150 | 0.8328 |
| 512, 256 // 128, 64 | 150 | 0.8328 |
| 512, 256 // 128| 150 | 0.8328 |
| 512 // 256, 128, 64 | 150 | 0.8328 |
| 512 // 256, 128 | 150 | 0.8328 |
| 512 // 256 | 150 | 0.8328 |
